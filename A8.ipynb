{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bedea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 4.0087\n",
      "Epoch 101/1000, Loss: 0.6118\n",
      "Epoch 201/1000, Loss: 0.1337\n",
      "Epoch 301/1000, Loss: 0.1183\n",
      "Epoch 401/1000, Loss: 0.1086\n",
      "Epoch 501/1000, Loss: 0.1018\n",
      "Epoch 601/1000, Loss: 0.0967\n",
      "Epoch 701/1000, Loss: 0.0928\n",
      "Epoch 801/1000, Loss: 0.0896\n",
      "Epoch 901/1000, Loss: 0.0870\n",
      "Epoch 1000/1000, Loss: 0.0849\n",
      "Test set accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, hidden_neurons, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        :param input_size: Number of input features\n",
    "        :param hidden_layers: Number of hidden layers (int)\n",
    "        :param hidden_neurons: Number of neurons in each hidden layer (int)\n",
    "        :param output_size: Number of output neurons (number of classes)\n",
    "        :param learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Input to first hidden layer\n",
    "        self.weights.append(np.random.randn(input_size, hidden_neurons) * np.sqrt(2. / input_size))\n",
    "        self.biases.append(np.zeros((1, hidden_neurons)))\n",
    "        \n",
    "        # Hidden layers weights\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(hidden_neurons, hidden_neurons) * np.sqrt(2. / hidden_neurons))\n",
    "            self.biases.append(np.zeros((1, hidden_neurons)))\n",
    "        \n",
    "        # Last hidden layer to output layer\n",
    "        self.weights.append(np.random.randn(hidden_neurons, output_size) * np.sqrt(2. / hidden_neurons))\n",
    "        self.biases.append(np.zeros((1, output_size)))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        :param X: Input data\n",
    "        :return: Output probabilities\n",
    "        \"\"\"\n",
    "        self.z_values = []\n",
    "        self.a_values = [X]\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.hidden_layers):\n",
    "            z = np.dot(self.a_values[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "            self.z_values.append(z)\n",
    "            self.a_values.append(a)\n",
    "        \n",
    "        # Output layer\n",
    "        z = np.dot(self.a_values[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z)\n",
    "        a = self.softmax(z)\n",
    "        self.a_values.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        :param y_true: True labels (one-hot encoded)\n",
    "        :param y_pred: Predicted probabilities\n",
    "        :return: Loss value\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass through the network to update weights and biases.\n",
    "        :param y_true: True labels (one-hot encoded)\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        grads_w = [None] * len(self.weights)\n",
    "        grads_b = [None] * len(self.biases)\n",
    "        \n",
    "        # Output layer error\n",
    "        delta = self.a_values[-1] - y_true  # shape (m, output_size)\n",
    "        \n",
    "        # Gradient for last layer weights and biases\n",
    "        grads_w[-1] = np.dot(self.a_values[-2].T, delta) / m\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in reversed(range(self.hidden_layers)):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * self.relu_derivative(self.z_values[i])\n",
    "            grads_w[i] = np.dot(self.a_values[i].T, delta) / m\n",
    "            grads_b[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        :param X_train: Training data features\n",
    "        :param y_train: Training data labels (one-hot encoded)\n",
    "        :param epochs: Number of training epochs\n",
    "        :param verbose: Whether to print loss during training\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = self.compute_loss(y_train, y_pred)\n",
    "            self.backward(y_train)\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data.\n",
    "        :param X: Input data\n",
    "        :return: Predicted class labels\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Example usage with Iris dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target.reshape(-1, 1)\n",
    "    \n",
    "    # One-hot encode the target labels\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_onehot = encoder.fit_transform(y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create neural network instance\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_layers = 1  # Can be changed to more layers if needed\n",
    "    hidden_neurons = 100\n",
    "    output_size = y_onehot.shape[1]\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    nn = NeuralNetwork(input_size, hidden_layers, hidden_neurons, output_size, learning_rate)\n",
    "    \n",
    "    # Train the network\n",
    "    nn.train(X_train, y_train, epochs=1000, verbose=True)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = nn.predict(X_test)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    print(f\"Test set accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
